---
title: "Modeling and Prediction for Movies"
output: 
  html_document: 
    fig_height: 4
    highlight: pygments
    theme: spacelab
---

## Setup

### Load packages

```{r load-packages, message = FALSE}
library(tidyverse)
library(statsr)
library(rstatix)
```

### Load data

```{r load-data}
load("movies.Rdata")
```



* * *

## Part 1: Data

###### The dataset is comprised of information from Rotten Tomatoes and IMBD about 651 movies produced and released before 2016. Aside from the size of this dataset, this data is generalizable due to the data-collection methods of Rotten Tomatoes and IMDB. Rotten Tomatoes employs a team of curators to read through thousands of movie and TV reviews every week and have a specific algorithm for accurately representing the data. IMDB pulls industry information through on-screen credits, press kits, interviews, and so forth while also allowing users to upload information. Their data consistently undergoes consistency checks to ensure accuracy and reliability. 

###### Inherently, bias is possible within the analysis of data. Selection bias may occur if any genres or ratings are underrepresented in the sample (for example, there are 9 Animation movies and 305 Drama movies). Confirmation bias may be present if the reviewers only selected or positively rated movies of interest to them. Due to the user-included information of IMDB, some selections may be subject to response bias if users submitted incorrect information. 

###### This dataset is comprehensive, randomly selected, is gathered from reliable sources, and compromises multiple decades and genres, so it is generalizable for the purposes of this project. Causality cannot be inferred since this data is observational and no random assignment was used for this data . This project is strictly for proof of concept. 

* * *

## Part 2: Research question

###### To predict what makes a movie popular, audience_score and imdb_rating (Rotten Tomatoes and IMDB, respectively) need to be considered. What variables are positively associated with audience_score? 

* * *

## Part 3: Exploratory Data Analysis

###### The main variables of interest are the response variables audience_score and imdb_rating. In addition to these, explore the relationships among these and the explanatory variables title_type, genre, runtime, thtr_rel_year (theatrical release year), and dvd_rel_year (DVD release year). These may be associated with the popularity of a given movie.

###### First, get a look at the structure of the dataset (651 observations (rows) over 32 variables (columns)); it is comprised of character, factor, numeric, and integer data types. A brief survey of which columns contain missing values (NAs) is helpful. The majority of missing values are found in the director, and actor1 through actor5 variables. The actor1 through actor5 are contain the names of the first, second, third, fourth, and fifth principal actors in a film. The specificity of these variables and the lack of principal actors in a film may explain these missing values.  Most variables do not contain missing values, however studio, dvd_rel_year, dvd_rel_month, dvd_rel_day all contain 8 missing values. This merits a closer look. 

```{r}
str(movies)
colSums(is.na(movies))
```

###### A common mistake would be assuming that there are only eight movies with missing data since there are several variables with 8 missing observations. That is why accurate filtering is important since there are 16 observations in total. Notice that if_all allows filtering over multiple rows with a given function by searching for a string of characters in the variable names. if_all was chosen rather than if_any because if_all is a stricter condition for filtering, requiring that (as seen above in the counts of missing values per variable) all variables with "dvd" in their name must have missing values rather than at least one of them.

```{r}
missing_subset <- movies %>%
    filter(
        is.na(studio) |
            if_all(contains("dvd"), is.na)
        )

missing_subset
```

###### Out of these observations, the majority were feature films either of the Drama, Action & Adventure, Art House & International, or Documentary genres. runtime spanned 74 132 minutes with an average runtime of 94.75 minutes and a median runtime of 93 minutes. audience_score (0 to 100) spanned 17 to 90 with an average score of 52.67 and a median score of 55.5. imdb_rating (0 to 10) spanned 3.1 to 8 with an average score of 5.83 and a median score of 6. imdb_num_votes (0 with in theory no maximum number of votes possible) spanned 486 to 30,886 with a mean number of votes of 5063.25 votes and a median number of votes of 3656 votes.

```{r}
missing_subset %>% count(title_type) %>% rename(`Title Type` = title_type, Total = n)
missing_subset %>% count(genre) %>% rename(Genre = genre, Total = n)

missing_subset %>%
  select(runtime, audience_score, imdb_rating, imdb_num_votes) %>%
  get_summary_stats()
```


###### Now consider the variables audience_score, imdb_rating, imdb_num_votes. title_type, genre, runtime, thtr_rel_year (theatrical release year), and dvd_rel_year (DVD release year) for the entire dataset. Out of these variables, title_type, genre, thtr_rel_year, and dvd_rel_year are categorical variables, and audience_score, imdb_rating, imdb_num_votes, and runtime are numerical variables. 

###### First, consider the categorical variables. Bar plots and frequency tables allow the exploration and visualization of categorical data. Among the full dataset, Feature Film was the most popular title type and nearly half of the movies were Dramas. Comedy and Action & Adventure were the second- and third-most popular genres. thr_rel_year (spanned 1970-2014) had releases primarily concentrated in 2000 onward. From dvd_rel_year(spanned 1991-2015), the early 2000s saw a majority of DVD releases. It appears to be more likely that movies had a different Theatrical and DVD release year.

```{r}
movies %>% count(title_type) %>% arrange(-n) %>% rename(`Title Type` = title_type, Total = n)
movies %>% count(genre) %>% arrange(-n) %>% rename(Genre = genre, Total = n)

movies %>% count(thtr_rel_year) %>% arrange(-n) %>% rename(`Theatrical Release Year` = thtr_rel_year, Total = n)
movies %>% count(dvd_rel_year) %>% arrange(-n) %>% rename(`DVD Release Year` = dvd_rel_year, Total = n)


ggplot(movies) +
  geom_bar(aes(x = thtr_rel_year)) +
  scale_x_continuous(breaks = seq(1970, 2015, 5)) +
  scale_y_continuous(breaks = seq(0, 35, 5)) +
  labs(
    title = "Movies by Theatrical Release Year",
    x = "Year",
    y = "Total"
  ) +
  theme_classic() +
  theme(plot.title = element_text(h = 0.5))

ggplot(movies) +
  geom_bar(aes(x = dvd_rel_year)) +
  scale_x_continuous(breaks = seq(1990,2015, 5)) +
  scale_y_continuous(breaks = seq(0, 75, 5)) +
  labs(
    title = "Movies by DVD Release Year",
    x = "Year",
    y = "Total"
  ) +
  theme_classic() +
  theme(plot.title = element_text(h = 0.5))


movies %>% count(thtr_rel_year == dvd_rel_year) %>% 
  rename(`Same Release Year - Theatrical and DVD` = `thtr_rel_year == dvd_rel_year`, Total = n)
```


###### Now to consider the numerical variables audience_score, imdb_rating, imdb_num_votes, and runtime. First, consider their summary statistics to get a better idea of the spread and measure of the data. audience_score spanned from 11 to 97, with an average score of 62.36 and a median score of 65. Slight left skewness is expected for audience_score since the mean is less than the median. imdb_rating spanned from 1.9 to 9 with an average rating of 6.49 and a median rating of 6.6. Slight left skewness is expected for imdb_rating. imdb_num_votes spanned from 180 votes to 893008 votes with a average number of votes of 57532.98 and a median number of votes of 15116. Right skewness is expected since the mean is greater than the median. Notice that runtime is missing one observation. runtime spanned from 39 minutes to 267 minutes with an average runtime of 105.82 minutes and a median runtime of 103 minutes. Almost no screw is expected since the mean and median values are near-identical. 

```{r}
movies %>%
  select(audience_score, imdb_rating, imdb_num_votes, runtime) %>%
  get_summary_stats()
```

###### Take a closer look at how title type affects audience_score. This can be viewed with a boxplot to capture the range of data and information about its quartiles. It's no surprise that a feature film would most likely have the highest number of audience_scores. Documentarie and tv-release films had significantly fewer observations, which may explain the difference in ranges. Feature Film had the largest spread in data, Documentary had the highest median, and TV Movie had the largest interquartile range (IQR). Unfortunately there is no variable available that contains the number of audience_scores on Rotten Tomatoes, just the audience_score per movie. 

```{r}
ggplot(movies) +
  geom_boxplot(aes(x = title_type, y = audience_score)) + 
  scale_y_continuous(breaks = seq(0, 100, 20), limits = c(0, 100)) +
  labs(
    title = "Rotten Tomatoes Audience Score by Title Type",
    x = "Title Type",
    y = "Audience Score",
    caption = "Documentary - 55 movies. Feature Film - 591 movies. TV Movie - 5 movies."
    ) +
  theme_classic() +
  theme(plot.title = element_text(h = 0.5))
```

###### To get a better idea of the above plot, it's worth looking at the same information with imdb_rating and imdb_num_votes. Consider the IMDB ratings on a scale of 1 to 10 (1 being the worst, 10 being the best). Documentary and Feature Film each have outliers (observations outside of the expected range). For boxplots, outliers are indetified by subtracting 1.5 times the IQR from the first quartile (median of "lower half" of data) and adding 1.5 times the IQR to the third quartile (median of "upper half" of data); these two values are where the respective minimum and maximum would be expected to lie. Points below or above these ones are the outliers. Documentary has one outlier below the minimum, and Feature Film has multiple outliers below the minimum and one outlier above the maximum. Pay attention to the spread of data. Documentary and Feature film appear to be symmetric about the median (middle bar in the boxplot), suggesting little to no skew. The median of TV Movie is the same as the third quartile, suggesting this title type has significant left skew. This may be due to such a low number of observations for TV Movie. 

###### Constructing a boxplot of the number of IMDB votes is near-impossible to interpret (included below for visualization purposes). From the boxplot produced, notice that all three title types have outliers. Very little can be interpreted from Documentary and TV Movie outliers, but Feature Film consistently had movies with a higher number of votes than expected by the range of the boxplot. Examining summary statistics rather than a boxplot would be a more approriate choice. Documentary IMDB votes spanned 180 votes to 39320 votes with an average of 5066.13 votes and a median of 1784 votes. Feature Film IMDB votes spanned 390 votes to 893008 votes with an average of 62.861.28 votes and a median of 17934 votes. TV Movie IMDB votes spanned 2289 votes to 11477 votes with an average of 4864 votes and a median of 3505 votes. 

```{r}
ggplot(movies) +
  geom_boxplot(aes(x = title_type, y = imdb_rating)) + 
  scale_y_continuous(breaks = seq(0, 10, 1), limits = c(0, 10)) +
  labs(
    title = "IMDB Rating by Title Type",
    x = "Title Type",
    y = "IMDB Rating",
    caption = "Documentary - 55 movies. Feature Film - 591 movies. TV Movie - 5 movies."
    ) +
  theme_classic() +
  theme(plot.title = element_text(h = 0.5))

ggplot(movies) +
  geom_boxplot(aes(x = title_type, y = imdb_num_votes)) + 
  scale_y_continuous(breaks = seq(0, 893008, 100000), limits = c(0, 893008), labels = scales::label_comma()) +
  labs(
    title = "IMDB Number of Votes by Title Type",
    x = "Title Type",
    y = "IMDB Number of Votes",
    caption = "Documentary - 55 movies. Feature Film - 591 movies. TV Movie - 5 movies."
    ) +
  theme_classic() +
  theme(plot.title = element_text(h = 0.5))

movies %>% 
  group_by(title_type) %>%
  select(title_type, imdb_num_votes) %>%
  get_summary_stats()
```

###### Consider the relationship between audience_score and imdb_rating and runtime. Among title types, the median appears to be concentrated around 100 minutes. Documentary and Feature Film had outliers; the Documentary outliers appeared below and above the minimum and maxium, and the Feature Film outliers appeared above the maximum. Documentary types may have the most symmetric distribution over the variable runtime. 

```{r}
ggplot(movies) +
  geom_boxplot(aes(x = title_type, y = runtime)) +
  scale_y_continuous(breaks = seq(0, 300, 50), limits = c(0, 300)) +
  labs(
    title = "Runtime by Title Type", 
    x = "Title Type",
    y = "Runtime",
    caption = "Documentary - 55 movies. Feature Film - 591 movies. TV Movie - 5 movies."
    ) +
  theme_classic() +
  theme(plot.title = element_text(h = 0.5))
  
```

###### Check to see if audience_score or imdb_rating are positively associated with runtime. This can be done using a linear model with the ratings as response variables and with runtime as an explanatory variable. Although the p-value was significant, it only explained about 3.3% of the variability in the model. Additionally, notice that the conditions of linear regression were violated: this is not a linear relationship (the residuals versus fitted plot curved up and down on the ends), errors didn't have constant variance (residuals versus fitted plot shows a blob between 60 and 70 rather than a random cloud of data points dispersed about the line y = 0), and the errors were not normally distributed (the tails of the normal Q-Q plot deviate after the -1 and +1 theoretical quantiles). Note: the documentary "The End of America" is excluded from the residuals plot below due to not having a value for runtime. 

```{r}
ggplot(movies, aes(x = runtime, y = audience_score)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) + 
  scale_x_continuous(breaks = seq(0, 300, 25), limits = c(0, 300)) +
  scale_y_continuous(breaks = seq(0, 100, 10), limits = c(0, 100)) +
  labs(
    title = "Rotten Tomatoes Audience Score versus Runtime", 
    x = "Runtime",
    y = "Audience Score",
    caption = "1 observation from runtime is missing."
  ) + 
  theme_classic() +
  theme(plot.title = element_text(h = 0.5))

model_1 <- lm(audience_score ~ runtime, data = movies)
summary(model_1)
plot(model_1$residuals ~ movies$runtime[movies$title != "The End of America"])
hist(model_1$residuals)
```

###### This model behaved slightly better. run_time had a significant p-value, yet it only explained about 7.2% of the variability. The conditions of linear modeling were still violated. The residuals versus fitted plot does not show a linear relationship due to the sharp decrease around 6 on the fitted values scale. The residuals still had a blob pattern rather than the desired random cloud, so constant variance was violated. The errors were normally distributed but had noticeable left skew. 

```{r}
ggplot(movies, aes(x = runtime, y = imdb_rating)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) + 
  scale_x_continuous(breaks = seq(0, 300, 25), limits = c(0, 300)) +
  scale_y_continuous(breaks = seq(0, 10, 1), limits = c(0, 10)) +
  labs(
    title = "IMDB Rating versus Runtime", 
    x = "Runtime",
    y = "IMDB Rating",
    caption = "1 observation from runtime is missing."
  ) + 
  theme_classic() +
  theme(plot.title = element_text(h = 0.5))

model_2 <- lm(imdb_rating ~ runtime, data = movies)
summary(model_2)
plot(model_2$residuals ~ movies$runtime[movies$title != "The End of America"])
hist(model_2$residuals)
```

###### Consider some other variables of interest: mpaa_rating, audience_rating ("Upright" - at least 60% of audience reviews were 3.5 stars out of 5 stars or more, "Spilled" - at least 60% of audience reviews were 3.5 stars out of 5 stars or higher), critics_rating ("Fresh" - less than 60% review score, "Certified Fresh" - specialized "Fresh" criteria that is harder to obtain, "Rotten" - less than 60% review score), critic_score (Rotten Tomatoes only, 0 to 100 scale), best_pic_nom (best picture nomination), best_pic_win (did the movie win best picture), and top200_box (was the movie on the Top 200 Box Office list from BoxOfficeMojo). 

###### Most ratings were concentrated among PG, PG-13, and R. TV Movies were exclusively R or Unrated, and the only entries for NC-17 were for Feature Films. From the boxplot for MPAA Ratings, be aware that G and NC-17 are small categories (19 and 2 observations, respectively). That may explain the higher medians. Unrated had 50 observations, but several movies scored lower than expected as shown by the outliers. PG, PG-13, and rated-R movies all appear to have median scores concentrated between 50 and 60, where PG had the highest IQR and PG-13 had the largest range. All ratings aside from NC-17 appeared to have left skew to some degree. 

```{r}
movies %>% count(mpaa_rating) %>% rename(`MPAA Rating` = mpaa_rating, Total = n)

ggplot(movies) +
  geom_bar(aes(x = mpaa_rating, fill = title_type), position = "dodge") +
  scale_y_continuous(breaks = seq(0, 300, 25), limits = c(0, 325)) +
  labs(
    title = "MPAA Ratings by Title Type",
    x = "MPAA Rating",
    y = "Total",
    fill = "Title Type"
  ) +
  theme_classic() +
  theme(plot.title = element_text(h = 0.5))

ggplot(movies) +
  geom_boxplot(aes(x = mpaa_rating, y = critics_score)) +
  scale_y_continuous(breaks = seq(0, 100, 20), limits = c(0, 100)) +
  labs(
    title = "Rotten Tomatoes Critic Score by MPAA Rating",
    x = "MPAA Rating",
    y = "Critic Score"
  ) +
  theme_classic() +
  theme(plot.title = element_text(h = 0.5))
```

###### Out of all movies rated by the audience, only 6 out of 11 genres had more Upright ratings than Spilled ratings. Among genres, Documentary movies had the highest percentage of Upright ratings by proportion. By nature, obtaining a Certified Fresh rating is more difficult than obtaining a Fresh rating. There are no genres in which there were more Certified Fresh ratings than Rotten ratings, but there are 3 genres in which there were more Fresh ratings than Rotten ratings.  Again, Documentary movies had the highest percentage of Fresh or higher ratings by proportion.

```{r}
genre_labels <- c("Action", "Animation", "Art", "Comedy", "Doc", "Drama", "Horror", "Music", "Mystery", "Other", "Sci-Fi")


ggplot(movies) +
  geom_bar(aes(x = genre, fill = audience_rating), position = "dodge") +
  scale_x_discrete(labels = genre_labels) + 
  scale_y_continuous(breaks = seq(0, 200, 20), limits = c(0, 200)) +
  labs(
    title = "Audience Rating by Genre",
    x = "Genre",
    y = "Total",
    fill = "Audience Rating"
    ) +
  theme_classic() +
  theme(plot.title = element_text(h = 0.5))

movies %>% 
  group_by(genre) %>% 
  reframe(
    `Number of Movies` = n(),
    `Percentage of Upright Audience Ratings` = round(sum(audience_rating == "Upright")/n() * 100, 2)
    ) %>%
  arrange(-`Percentage of Upright Audience Ratings`) %>%
  rename(Genre = genre)


ggplot(movies) +
  geom_bar(aes(x = genre, fill = critics_rating), position = "dodge") +
  scale_x_discrete(labels = genre_labels) + 
  scale_y_continuous(breaks = seq(0, 140, 20), limits = c(0, 140)) +
  labs(
    title = "Critic Rating by Genre",
    x = "Genre",
    y = "Total",
    fill = "Critic Rating"
    ) +
  theme_classic() +
  theme(plot.title = element_text(h = 0.5))
  
movies %>% 
  group_by(genre) %>% 
  reframe(
    `Number of Movies` = n(),
    `Percentage of Fresh or Higher Critic Ratings` = round(sum(critics_rating != "Rotten")/n() * 100, 2)
    ) %>%
  arrange(-`Percentage of Fresh or Higher Critic Ratings`) %>%
  rename(Genre = genre)
```

###### The only genres to be nominated for best picture were Comedy, Drama, Mystery & Suspense, and Other. The same genres were also present on the Top 200 Box Office list. Out of those categories, only Other did not win best picture. The only genre that was on the Top 200 Box Office list and won best picture was Drama (the movie was *Titanic*). 

```{r}
movies %>% 
  group_by(genre) %>% 
  select(genre, best_pic_nom) %>% 
  count(best_pic_nom) %>% 
  arrange(genre) %>%
  rename(Genre = genre, `Best Picture Nomination` = best_pic_nom, Total = n)

movies %>% 
  group_by(genre) %>% 
  select(genre, best_pic_win) %>% 
  count(best_pic_win) %>% 
  arrange(genre) %>%
  rename(Genre = genre, `Did it win best picture?` = best_pic_win, Total = n)

movies %>% 
  group_by(genre) %>% 
  select(genre, top200_box) %>%
  count(top200_box) %>% 
  arrange(genre) %>%
  rename(Genre = genre, `Was it on the Top 200 Box Office list?` = top200_box, Total = n)

movies %>% 
  group_by(genre) %>% 
  select(genre, top200_box, best_pic_win) %>%
  count(top200_box == "yes" & best_pic_win == "yes") %>% 
  arrange(genre) %>%
  rename(
    Genre = genre, 
    `Was it on the Top 200 Box Office list and won best picture?` = `top200_box == "yes" & best_pic_win == "yes"`, 
    Total = n
    )

movies %>% filter(top200_box == "yes" & best_pic_win == "yes") %>% select(title)
```


* * *

## Part 4: Modeling

###### To construct a model for explaining movie popularity through audience_score and imdb_rating, it has to be decided which variables are the best for explaining the relationship. This can be done with forward selection (adding variables to the model) or backward elimination (removing variables from the model) among other techniques. Backward elimination will be done here. In backward elimination, start with a full model and remove variables based on either improved adjusted R-squared or p-value. The adjusted R-squared method requires removing each variable one at a time over and over until the adjusted R-squared does not improve. The p-value method works similarly but removing the largest p-value instead. The adjusted R-squared method results in a stronger prediction power by prioritizing the highest adjusted R-squared value, but the p-value method results in the simplest model with a slightly lower prediction power (parsimony). Here parsimony is favored, so the backward elimination p-value method is used.

###### It is obvious that the variables imdb_url and rt_url will not offer anything to the model since they're just URLs to the website entry for each movie. Don't use title for similar reasons. If absolutely all variables are included, then the adjusted R-squared will be 1 (all variability is explained by the model) but the model will be overfitted (it will look "too busy" and be hard to determine trends). Instead create a full model for audience_score using title_type, genre, runtime, mpaa_rating, thtr_rel_year, imdb_rating, imdb_num_votes, critic_score, critics_rating, audience_rating, best_pic_nom, best_pic_win, and top200_box.  

```{r}
full_model <- lm(audience_score ~ title_type + genre + runtime + mpaa_rating + thtr_rel_year + imdb_rating + imdb_num_votes + critics_score + critics_rating + audience_rating + best_pic_nom + best_pic_win + top200_box, data = movies)
```

###### Before running the model, the variables need to be checked for multicollinearity. This happens when independent variables are strongly correlated to each other to the extent that the model is overfitted and as a result it is difficult to interpret. When multicollinearity is present, the variances of the variables are inflated; one way to check for multicollinearity is to check the Variance Inflation Factor with the function vif. If the VIF is measured as greater than 5, there is a suffucient amount of multicollinearity present. Multicollinearity can be handled by dropping the strongly correlated variables, averaging them, or reducing dimensionality using principal component analysis (PCA). Here, the strongly correlated variables will be dropped. 

###### Note that there are three measures present for the variables: GVIF, Df, and GVID^(1/(2*Df)). Since categorical variables are included in the model, the measure GVIF (generalized VIF) is included to calculate a VIF that would be the same regardless of the factor level. Df (degrees of freedom) represents the number of independent pieces to calculate a statistic. The final column represents the generalized standard inflation factor, takign the GVIF  to the fractional power of 1/2 times the Df column for something more generalizable should the factors have more than two levels. For that reason, aGSIF will be the chosen measure here. Typically a cutoff value for GVIF is to drop variables with a GVIF of 5 or higher. When using aGSIF, take the square root of the GVIF threshold, so if aGSIF is greater than 2.2361, drop the variable. Based on that, critics_score should be dropped from the model.

```{r}
car::vif(full_model)

sqrt(5)
```

###### Initially this model performs fairly well, explaining 88.53% of the total variance. First, drop the variable with the highest p-value. The Other genre has the highest p-value, but genre has levels that are significant, so genre cannot be dropped. The highest p-value is mpaa_rating, which has insignificant p-values on all levels. Drop this variable and refit the model.

```{r}
model <- lm(audience_score ~ title_type + genre + runtime + mpaa_rating + thtr_rel_year + imdb_rating + imdb_num_votes + critics_rating + audience_rating + best_pic_nom + best_pic_win + top200_box, data = movies)

summary(model)
```

###### Doing the process again, the next variable to be dropped is critics_rating, which is insignificant on all levels and has the highest p-value on the level Fresh.

```{r}
model1 <- lm(audience_score ~ title_type + genre + runtime + thtr_rel_year + imdb_rating + imdb_num_votes + critics_rating + audience_rating + best_pic_nom + best_pic_win + top200_box, data = movies)

summary(model1)
```

###### The next highest p-value belongs to genre, however there is one genre level with a significant p-value (Mystery & Suspense). Thus, drop title_type instead.

```{r}
model2 <- lm(audience_score ~ title_type + genre + runtime + thtr_rel_year + imdb_rating + imdb_num_votes + audience_rating + best_pic_nom + best_pic_win + top200_box, data = movies)

summary(model2)
```

###### Next, drop top200_box.

```{r}
model3 <- lm(audience_score ~ genre + runtime + thtr_rel_year + imdb_rating + imdb_num_votes + audience_rating + best_pic_nom + best_pic_win + top200_box, data = movies)

summary(model3)
```

###### Drop the variable best_pic_win. 

```{r}
model4 <- lm(audience_score ~ genre + runtime + thtr_rel_year + imdb_rating + imdb_num_votes + audience_rating + best_pic_nom + best_pic_win, data = movies)

summary(model4)
```

###### Drop imdb_num_votes next.

```{r}
model5 <- lm(audience_score ~ genre + runtime + thtr_rel_year + imdb_rating + imdb_num_votes + audience_rating + best_pic_nom, data = movies)

summary(model5)
```

###### Now, drop thtr_rel_year.

```{r}
model6 <- lm(audience_score ~ genre + runtime + thtr_rel_year + imdb_rating + audience_rating + best_pic_nom, data = movies)

summary(model6)
```

###### Notice that runtime is no longer a significant predictor of audience_score. Drop runtime from the model.

```{r}
model7 <- lm(audience_score ~ genre + runtime + imdb_rating + audience_rating + best_pic_nom, data = movies)

summary(model7)
```

###### Similarly best_pic_nom is no longer a significant predictor. Drop best_pic_nom.

```{r}
model8 <- lm(audience_score ~ genre + imdb_rating + audience_rating + best_pic_nom, data = movies)

summary(model8)
```

###### There are no variables with insignificant p-values (if categorical on all levels) left. The backward elimination is complete. Notice that the adjusted R-squared is now 88.47% (this is the percentage of total variance explained by the model) is lower than the initial adjusted R-squared of 88.53%. This is the trade-off with choosing a parsimonious model.

```{r}
final_model <- lm(audience_score ~ genre + imdb_rating + audience_rating, data = movies)

summary(final_model)
```


###### It needs to be ensured that the conditions for multiple linear regression are met: a linear relationship between numerical variables, nearly normal residuals with mean 0, and constant variability of residuals.

###### The only numerical value in our final model is imdb_rating. To check for linearity, plot the residuals against imdb_rating. We see a complete random scatter around the line y = 0. The relationship is linear.

```{r}
plot(
  final_model$residuals ~ movies$imdb_rating, 
  main = "Residuals of Final Model against IMDB Rating", 
  xlab = "IMDB Rating",
  ylab = "Residuals"
  )
abline(h = 0)
```

###### The residuals are normally distributed with minimal deviation from the Q-Q line. The mean of residuals is etremely close to 0. The condition of normality is satisfied.

```{r}
hist(final_model$residuals)
qqnorm(final_model$residuals)
qqline(final_model$residuals)

mean(final_model$residuals)
```

###### Note that clusters are present due to the presence of categorical variables in the model (most likely due to audience_rating since it's a factor of two levels). Since both clusters have positive and negative residuals, this is fine--neither cluster overestimates or underestimates the result. What's more important is that the residuals are randomly scattered about the line y = 0 with constant width. The condition of constant variance is satisfied.

```{r}
plot(
  final_model$residuals ~ final_model$fitted, 
  main = "Residuals of Final Model against Fitted Values of Final Model",
  xlab = "Fitted Values",
  ylab = "Residuals"
  )
abline(h = 0)

plot(
  abs(final_model$residuals) ~ final_model$fitted, 
  main = "Absolute Value of Residuals of Final Model against \n Fitted Values of Final Model",
  xlab = "Fitted Values",
  ylab = "Absolute Value of Residuals"
  )
abline(h = 0)
```


* * *

## Part 5: Prediction

###### Consider the movie *Sing* released in 2016. This movie had an audience rating of "Upright," and an IMDB of 7.1, and a genre of "Animation," "Comedy," and "Musical & Performing Arts." This produces three predicted audience scores based on genre including a 95% confidence interval. If the genre is Animation, then the predicted audience score is 80.98 with a possible range of 76.44 to 85.53. If the genre is Comedy, then the predicted audience score is 78.87 with a possible range of 77.20 to 80.54. If the genre is Musical & Performing Arts, then the predicted audience score is 79.90 with a possible range of 76.00 to 83.80. 

```{r}
pred_data <- tibble(
  audience_rating = rep("Upright", times = 3),
  imdb_rating = rep(7.1, times = 3),
  genre = c("Animation", "Comedy", "Musical & Performing Arts")
)

predict(final_model, pred_data)
predict(final_model, pred_data, interval = "confidence")
```

* * *

## Part 6: Conclusion

###### According to Rotten Tomatoes, the audience rating for *Sing* is 73%, which is slightly below the widest confidence interval (Animation). This could be due to many factors, for example the presence of primarily categorical variables in the final model or the choice of parsimony over higher adjusted R-squared. A better possibility is that the true relationship is nonlinear. audience_score is a count variable on a scale of whole numbers from 0 to 100 signifiying a given percentage, so a generalized linear model used for count variables is probably a better choice. This is not demonstrated above since the project topic is multiple linear regression. 

###### Through the backward elimination p-value method for model selection, it has been shown that genre, IMDB rating, and Rotten Tomatoes audience rating are sufficient predictors for popular movies. Additionally, *Titanic* was the only film to be on the BoxOfficeMojo Top 200 Box Office list and win best picture.

###### Note: A follow-up project will be done using this data to show how principal component analysis can be used to in model selection rather than manual methods. 